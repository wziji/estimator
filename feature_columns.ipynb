{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_name = ['Sepallength', 'Sepalwidth', 'Petallength', 'Petalwidth', 'label']\n",
    "data = pd.read_csv('iris_training.csv', sep=\",\")\n",
    "data.columns = columns_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepallength</th>\n",
       "      <th>Sepalwidth</th>\n",
       "      <th>Petallength</th>\n",
       "      <th>Petalwidth</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepallength  Sepalwidth  Petallength  Petalwidth  label\n",
       "0          6.4         2.8          5.6         2.2      2\n",
       "1          5.0         2.3          3.3         1.0      1\n",
       "2          4.9         2.5          4.5         1.7      2\n",
       "3          4.9         3.1          1.5         0.1      0\n",
       "4          5.7         3.8          1.7         0.3      0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_map = {\"sepal.length\": 'Sepallength',\n",
    "         \"sepal.width\": 'Sepalwidth',\n",
    "         \"petal.length\": 'Petallength',\n",
    "         \"petal.width\": 'Petalwidth',\n",
    "         \"variety\": 'label'}\n",
    "\n",
    "data = data.rename(index=str, columns=col_map)\n",
    "\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': array([2, 1, 2, 0, 0, 0, 0, 2, 1, 0, 1, 1, 0, 0, 2, 1, 2, 2, 2, 0, 2, 2,\n",
      "       0, 2, 2, 0, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 0,\n",
      "       0, 2, 0, 2, 0, 2, 0, 1, 1, 0, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2,\n",
      "       0, 2, 2, 0, 0, 1, 0, 2, 2, 0, 1, 1, 1, 2, 0, 1, 1, 1, 2, 0, 1, 1,\n",
      "       1, 0, 2, 1, 0, 0, 2, 0, 0, 2, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "       2, 1, 0, 2, 0, 1, 1, 0, 0, 1])}\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = data.iloc[:, 0:4], data.iloc[:, -1:]\n",
    "features = {key: np.array(value) for key, value in dict(x_train).items()}\n",
    "target = {key: np.array(value) for key,value in dict(y_train).items()}\n",
    "\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. input_layer 层\n",
    "\n",
    "```python\n",
    "\n",
    "tf.feature_column.input_layer(\n",
    "    features,\n",
    "    feature_columns,\n",
    "    weight_collections=None,\n",
    "    trainable=True,\n",
    "    cols_to_vars=None,\n",
    "    cols_to_output_tensors=None\n",
    ")\n",
    "```\n",
    "\n",
    "使用 input_layer 作为 model 的一个 input layer。\n",
    "\n",
    "+ features: 字典，最主要的是 dict的key一定要与 feature_columns的key一致，后续才能 才能根据key进行匹配。\n",
    "+ feature_columns：改参数必须是 继承于DenseColumn的 numeric_column, embedding_column, bucketized_column, indicator_column。如果feature是类别的，那么必须先用embedding_column or indicator_column封装一下使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Numeric feature columns\n",
    "\n",
    "## 2.1 numeric_column\n",
    "\n",
    "```python\n",
    "tf.feature_column.numeric_column(\n",
    "    key,\n",
    "    shape=(1,),\n",
    "    default_value=None,\n",
    "    dtype=tf.dtypes.float32,\n",
    "    normalizer_fn=None\n",
    ")\n",
    "```\n",
    "\n",
    "参数说明：\n",
    "\n",
    "+ key: 唯一确定输入特征的字符串，也就是说该字段用来标识这个特征。\n",
    "+ shape：该特征的形状。\n",
    "+ default_value 默认值。\n",
    "+ dtype：数据类型。\n",
    "+ normalizer_fn：做一些normalization操作，比如标准化。normalizer 函数 input: tensor；output:tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_NumericColumn(key='Sepallength', shape=(1,), default_value=(0,), dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='Sepalwidth', shape=(1,), default_value=(0,), dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='Petallength', shape=(1,), default_value=(0,), dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='Petalwidth', shape=(1,), default_value=(0,), dtype=tf.float32, normalizer_fn=None)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns = [tf.feature_column.numeric_column(key, default_value=0) for key in x_train.keys()]\n",
    "\n",
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_layer_22/concat:0\", shape=(120, 4), dtype=float32)\n",
      "[[5.6 2.2 6.4 2.8]\n",
      " [3.3 1.  5.  2.3]\n",
      " [4.5 1.7 4.9 2.5]\n",
      " [1.5 0.1 4.9 3.1]\n",
      " [1.7 0.3 5.7 3.8]]\n"
     ]
    }
   ],
   "source": [
    "# 使用 input_layer 作为 model的一个 input layer\n",
    "\n",
    "inn = tf.feature_column.input_layer(features, feature_columns)\n",
    "print(inn)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(inn[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean is 5.845, std is 0.8685784774150068\n"
     ]
    }
   ],
   "source": [
    "# 使用 normalizer_fn, 标准化\n",
    "\n",
    "mean = x_train['Sepallength'].mean()\n",
    "std = x_train['Sepallength'].std()\n",
    "print('mean is {}, std is {}'.format(mean, std))\n",
    "\n",
    "def normalization(mean, std):\n",
    "    def normalizer_fn(x):\n",
    "        return (x-mean) / std   \n",
    "    return normalizer_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_NumericColumn(key='Sepallength', shape=(1,), default_value=(0,), dtype=tf.float32, normalizer_fn=<function normalization.<locals>.normalizer_fn at 0x1520ec1bf950>)\n",
      "========================\n",
      "\n",
      "Tensor(\"input_layer_23/concat:0\", shape=(120, 1), dtype=float32)\n",
      "========================\n",
      "\n",
      "[[ 0.6389751 ]\n",
      " [-0.97285396]\n",
      " [-1.0879846 ]\n",
      " [-1.0879846 ]\n",
      " [-0.16693944]]\n"
     ]
    }
   ],
   "source": [
    "feature = tf.feature_column.numeric_column('Sepallength', \n",
    "                                           default_value=0, \n",
    "                                           normalizer_fn=normalization(mean, std))  \n",
    "\n",
    "print(feature)\n",
    "print(\"========================\\n\")\n",
    "\n",
    "in_Sepallength = tf.feature_column.input_layer(features, [feature])\n",
    "print(in_Sepallength)\n",
    "print(\"========================\\n\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(in_Sepallength[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. categorical feature columns\n",
    "\n",
    "## 3.1 tf.feature_column.categorical_column_with_identity\n",
    "\n",
    "可以理解为将类型为int的特征，转换为one-hot类型的dense tensor.\n",
    "\n",
    "```python\n",
    "tf.feature_column.categorical_column_with_identity(\n",
    "    key,\n",
    "    num_buckets,\n",
    "    default_value=None\n",
    ")\n",
    "```\n",
    "\n",
    "首先它返回的是唯一值的类别 column (categorical column)\n",
    "\n",
    "+ key：参数同上。\n",
    "+ num_buckets: 限制输入输出的数据范围。范围是[0, num_buckets)，还有就是使用这个函数，对应的feature value必须是int类型。\n",
    "+ default_value: 默认值，该值必须在 [0, num_buckets)范围内。如果不设置默认值，则会抛出异常（如果数据不在[0, num_buckets)范围内）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': [0, 1, 2, 3, 4, 4, 6, 7, 8, 9], 'cat': ['test', 'train', 'eval', 'train', 'train', 'test', 'eval', 'eval', 'test', 'train']}\n"
     ]
    }
   ],
   "source": [
    "features_new  = {'id': [0, 1, 2, 3, 4, 4, 6, 7, 8, 9],\n",
    "                 'cat': ['test', 'train', 'eval', 'train', 'train', \n",
    "                                  'test', 'eval', 'eval', 'test', 'train']}\n",
    "\n",
    "print(features_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = tf.feature_column.categorical_column_with_identity('id', 10)\n",
    "ci_int = tf.feature_column.indicator_column(ci)\n",
    "\n",
    "# 为什么添加这个步骤呢？因为 input_layer的入参是有要求的，如上所说必须是继承于DenseColumn的类型,详看下面的解释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "in_ci = tf.feature_column.input_layer(features_new, [ci_int])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(in_ci))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 tf.feature_column.indicator_column\n",
    "\n",
    "tf.feature_column.indicator_column(categorical_column)\n",
    "\n",
    "+ 参数：必须是categorical_column，如上可知，只有categorical_column_with*, crossed_column以及bucketized_column 类型的column才可以使用该函数。\n",
    "+ 作用：将categorical_column表示成 multi-hot形式的 dense tensor。\n",
    "\n",
    "下面官方还特意提到了一点就是：indicator_column 可以将任意的 categorical_column_with* 作为参数（同我上面说的一致），但是，如果说 buckets/unique（分箱，或者 ont-hot）的值比较大的时候，还是建议使用embedding_column。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 tf.feature_column.categorical_column_with_vocabulary_file\n",
    "\n",
    "```python\n",
    "tf.feature_column.categorical_column_with_vocabulary_file( key,\n",
    "    vocabulary_file,\n",
    "    vocabulary_size=None,\n",
    "    num_oov_buckets=0,\n",
    "    default_value=None,\n",
    "    dtype=tf.dtypes.string\n",
    "    )\n",
    "```\n",
    "\n",
    "+ 作用：将表示类别的feature转换为Categorical Column tensor。\n",
    "+ 参数：\n",
    "    + key: 同上。\n",
    "    + vocabulary_file：字典文件。\n",
    "    + vocabulary_size：字典文件中元素个数，该值不能大于字典元素数；如果该值小于字典元素数，则字典中后续的元素将被忽略。默认情况下，该值会被自动设置为字典元素数。\n",
    "    + num_oov_buckets：就是说如果value值超出了字典，后续应该用什么值来填充，基于hash算法，填充值在 [vocabulary_size, vocabulary_size + num_oov_buckets)范围内。注意该值与 default_value 不共存(不能同时存在，否则会报错)。\n",
    "    + default_value：默认-1，就是说当 feature value的值不在字典中时，也就是oov，使用一个整数默认值来填充。\n",
    "    + dtype：feature type，目前仅支持 string 和 integer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "echo \"train\" >> vocabulary_file.csv\n",
    "echo \"test\" >> vocabulary_file.csv\n",
    "echo \"eval\" >> vocabulary_file.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IndicatorColumn(categorical_column=_VocabularyFileCategoricalColumn(key='cat', vocabulary_file='vocabulary_file.csv', vocabulary_size=2, num_oov_buckets=3, dtype=tf.string, default_value=-1))\n",
      "Tensor(\"input_layer_25/concat:0\", shape=(10, 5), dtype=float32)\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "cate_vf = tf.feature_column.categorical_column_with_vocabulary_file('cat',\n",
    "                vocabulary_file='vocabulary_file.csv',\n",
    "                vocabulary_size=2,\n",
    "                num_oov_buckets=3)\n",
    "\n",
    "ind_cat = tf.feature_column.indicator_column(cate_vf)\n",
    "print(ind_cat)\n",
    "\n",
    "inp_cat = tf.feature_column.input_layer(features_new, [ind_cat])\n",
    "print(inp_cat)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 此处必须使用 tf.tables_initializer来初始化 lookuptable\n",
    "    sess.run(tf.tables_initializer())\n",
    "    print(sess.run(inp_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 tf.feature_column.categorical_column_with_vocabulary_list\n",
    "\n",
    "```python\n",
    "tf.feature_column.categorical_column_with_vocabulary_list(key,\n",
    "    vocabulary_list,\n",
    "    dtype = None,\n",
    "    default_value = -1,\n",
    "    num_oov_buckets = 0\n",
    "    )\n",
    "```\n",
    "\n",
    "+ 除了vocabulary_list参数外，其它都与categorical_column_with_vocabulary_file一致。\n",
    "+ vocabulary_list：指定字典列表，注意顺序，解析后的数据是按照 list 索引决定的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_VocabularyListCategoricalColumn(key='cat', vocabulary_list=('test', 'train', 'eval'), dtype=tf.string, default_value=1, num_oov_buckets=0)\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "cate_vl = tf.feature_column.categorical_column_with_vocabulary_list('cat',\n",
    "                vocabulary_list = ['test', 'train', 'eval'],\n",
    "                default_value = 1)\n",
    "\n",
    "print(cate_vl)\n",
    "\n",
    "ind_cat_l = tf.feature_column.indicator_column(cate_vl)\n",
    "inp_cat_l = tf.feature_column.input_layer(features_new, [ind_cat_l])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 在此处必须使用 tf.tables_initializer来初始化 lookuptable\n",
    "    sess.run(tf.tables_initializer())\n",
    "    print(sess.run(inp_cat_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_VocabularyListCategoricalColumn(key='cat', vocabulary_list=('test', 'train', 'eval'), dtype=tf.string, default_value=2, num_oov_buckets=0)\n",
      "\n",
      "\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "features_multi = {'id': [0, 1, 2, 3, 4, 4, 6, 7, 8, 9], \n",
    "                  'cat': ['test train', 'train eval', 'eval', 'train', \n",
    "                          'train', 'test', 'eval', 'eval', 'test', 'train']}\n",
    "\n",
    "cate_multi = tf.feature_column.categorical_column_with_vocabulary_list('cat',\n",
    "                vocabulary_list = ['test', 'train', 'eval'],\n",
    "                default_value = 2)\n",
    "\n",
    "print(cate_multi)\n",
    "print(\"\\n\")\n",
    "\n",
    "ind_multi=tf.feature_column.indicator_column(cate_multi)\n",
    "inp_multi=tf.feature_column.input_layer(features_multi,[ind_multi])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 在此处必须使用 tf.tables_initializer来初始化 lookuptable\n",
    "    sess.run(tf.tables_initializer())\n",
    "    print(sess.run(inp_multi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 tf.feature_column.categorical_column_with_hash_bucket\n",
    "\n",
    "```python\n",
    "tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    key,\n",
    "    hash_bucket_size,\n",
    "    dtype=tf.dtypes.string\n",
    "    )\n",
    "```\n",
    "\n",
    "+ 作用：利用hash算法，将特征值类型为 integer/string 的特征进行分箱操作。\n",
    "+ 参数：\n",
    "    + key：同上。\n",
    "    + hash_bucket_size：int类型且>1。\n",
    "    + dtype：特征的类型，目前仅支持 string 和integer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "cate_hb = tf.feature_column.categorical_column_with_hash_bucket('id',\n",
    "                hash_bucket_size = 5,\n",
    "                dtype = tf.int64)\n",
    "\n",
    "ind_cat_hb = tf.feature_column.indicator_column(cate_hb)\n",
    "inp_cat_hb = tf.feature_column.input_layer(features_new, [ind_cat_hb])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(inp_cat_hb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 tf.feature_column.bucketized_column\n",
    "\n",
    "```python\n",
    "tf.feature_column.bucketized_column(\n",
    "    source_column,\n",
    "    boundaries\n",
    "    )\n",
    "```\n",
    "\n",
    "+ 作用：分箱目的是离散化数据，比如将年龄划分为：少年，青年，壮年以及老年。\n",
    "+ 参数：\n",
    "    + source_column：numeric_column。\n",
    "    + boundaries：边界列表，形成 左闭右开 区间，比如 [1,2,3]，那么拆分后的边界将是 (-inf,1), [1,2), [2,3), [3,+inf) 四个区间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_BucketizedColumn(source_column=_NumericColumn(key='id', shape=(1,), default_value=(0,), dtype=tf.float32, normalizer_fn=None), boundaries=(3, 5, 8))\n",
      "\n",
      "**********\n",
      "Tensor(\"input_layer_32/concat:0\", shape=(10, 4), dtype=float32)\n",
      "**********\n",
      "\n",
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "bc_fc = tf.feature_column.numeric_column('id', default_value = 0)\n",
    "bucket = tf.feature_column.bucketized_column(bc_fc, boundaries = [3, 5, 8])\n",
    "\n",
    "print(bucket)\n",
    "\n",
    "input_bucket=tf.feature_column.input_layer(features_new, [bucket])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"\\n**********\")\n",
    "    print(input_bucket)\n",
    "    print(\"**********\\n\")\n",
    "    print(sess.run(input_bucket))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 插一个方法 make_parse_example_spec\n",
    "\n",
    "如果之前做过 tf 的serving，那么很有可能使用到 tf.FixedLenFeature / tf.VarLenFeature 这类方法，定义时，需要指定shape。\n",
    "如果对这块理解不到位，那么shape很容易写错，导致 export model 失败。但是现在有了 make_parse_example_spec 方法，可以方便的为您进行转换。\n",
    "\n",
    "如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': FixedLenFeature(shape=(1,), dtype=tf.float32, default_value=(0,))}\n"
     ]
    }
   ],
   "source": [
    "tt = tf.feature_column.make_parse_example_spec([bucket])\n",
    "\n",
    "print(tt)\n",
    "\n",
    "#with tf.Session() as sess:\n",
    "#    print(sess.run(tt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 tf.feature_column.crossed_column\n",
    "\n",
    "```python\n",
    "tf.feature_column.crossed_column(\n",
    "    keys,\n",
    "    hash_bucket_size,\n",
    "    hash_key=None\n",
    "    )\n",
    "```\n",
    "\n",
    "+ 说明：该函数的作用主要是通过笛卡尔积产生新的特征，然后经过hash算法产生最终特征：Hash(特征的笛卡尔积) % hash_bucket_size。\n",
    "+ 参数：\n",
    "    + keys：指明哪些 feature 需要进行合并形成新的特征，注意，该值的类型可以是 string(也就是数据集的column)，也可以是经过变换后的 categoricalColumn，但是不支持 hashed categorical。\n",
    "    + hash_bucket_size：分箱的长度，可以理解为合并后数据的列数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_layer_33/concat:0\", shape=(?, 5), dtype=float32)\n",
      "\n",
      "\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "id_x_cat = tf.feature_column.crossed_column(['id', 'cat'], 5)\n",
    "\n",
    "fc_ic = tf.feature_column.indicator_column(id_x_cat)\n",
    "inp_cc = tf.feature_column.input_layer(features_new, [fc_ic])\n",
    "\n",
    "print(inp_cc)\n",
    "print(\"\\n\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(inp_cc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': VarLenFeature(dtype=tf.string), 'cat': VarLenFeature(dtype=tf.string)}\n"
     ]
    }
   ],
   "source": [
    "# 关于合并后的特征，serving时，可以看下效果\n",
    "\n",
    "features = tf.feature_column.make_parse_example_spec([id_x_cat])\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 tf.feature_column.embedding_column\n",
    "\n",
    "```python\n",
    "\n",
    "tf.feature_column.embedding_column(\n",
    "    categorical_column,\n",
    "    dimension,\n",
    "    combiner='mean',\n",
    "    initializer=None,\n",
    "    ckpt_to_load_from=None,\n",
    "    tensor_name_in_ckpt=None,\n",
    "    max_norm=None,\n",
    "    trainable=True\n",
    "    )\n",
    "```\n",
    "\n",
    "+ 作用：该方法和indicator_column一样，只接受 categorical_column，目的是将稀疏矩阵转换为稠密矩阵，word2vec。\n",
    "+ 参数：\n",
    "    + categorical_column：入参categorical_column_with* 的返回。\n",
    "    + dimension：embedding 的维度，一般计算规则是类别开4次方，但是也可以根据需要自行设置。\n",
    "    + combiner：多个vector的组合方式，有 mean(default), sqrtn以及sum。\n",
    "    + initializer：embedding matrix 的初始化值，默认均值0，标准差 1/sqrt(dimension)的tf.truncated_normal_initializer。\n",
    "    + ckpt_to_load_from以及tensor_name_in_ckpt：主要是为了使用pre-trained embedding matrix。\n",
    "    + max_norm：if not 'None'，则使用l2归一化。\n",
    "    + trainable：是否可训练的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': [0, 1, 2, 3, 4, 4, 6, 7, 8, 9], 'cat': ['test', 'train', 'eval', 'train', 'train', 'test', 'eval', 'eval', 'test', 'train']}\n",
      "\n",
      "\n",
      "_VocabularyListCategoricalColumn(key='cat', vocabulary_list=('test', 'train', 'eval'), dtype=tf.string, default_value=1, num_oov_buckets=0)\n"
     ]
    }
   ],
   "source": [
    "print(features_new)\n",
    "print(\"\\n\")\n",
    "\n",
    "cate_eb = tf.feature_column.categorical_column_with_vocabulary_list('cat',\n",
    "                vocabulary_list=['test', 'train', 'eval'],                                                                 \n",
    "                default_value=1)\n",
    "\n",
    "print(cate_eb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_EmbeddingColumn(categorical_column=_VocabularyListCategoricalColumn(key='cat', vocabulary_list=('test', 'train', 'eval'), dtype=tf.string, default_value=1, num_oov_buckets=0), dimension=3, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x151f5dbe5cf8>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)\n",
      "\n",
      "[[-0.78481716  0.6328265   0.6165252 ]\n",
      " [-0.78481716  0.6328265   0.6165252 ]\n",
      " [-0.14772381 -0.2884772  -0.597014  ]\n",
      " [-0.78481716  0.6328265   0.6165252 ]\n",
      " [-0.78481716  0.6328265   0.6165252 ]\n",
      " [ 0.5468127   1.0355408   0.01168238]\n",
      " [-0.14772381 -0.2884772  -0.597014  ]\n",
      " [-0.14772381 -0.2884772  -0.597014  ]\n",
      " [ 0.5468127   1.0355408   0.01168238]\n",
      " [-0.78481716  0.6328265   0.6165252 ]]\n",
      "\n",
      "(?, 3)\n"
     ]
    }
   ],
   "source": [
    "eb_col = tf.feature_column.embedding_column(cate_eb, 3)\n",
    "\n",
    "print(eb_col)\n",
    "print()\n",
    "\n",
    "inp_eb=tf.feature_column.input_layer(features_multi,[eb_col])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 在此处必须使用 tf.tables_initializer来初始化 lookuptable\n",
    "    sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    print(sess.run(inp_eb))\n",
    "    print()\n",
    "    print(inp_eb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考\n",
    "\n",
    "+ [https://github.com/AlbertBJ/tensorflow-summary/blob/master/feature_column/feature_column.ipynb](https://github.com/AlbertBJ/tensorflow-summary/blob/master/feature_column/feature_column.ipynb)\n",
    "\n",
    "+ [https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/feature_columns.md](https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/feature_columns.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
